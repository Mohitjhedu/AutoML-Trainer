{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e22980fc-55e8-4735-ac0e-cbbfe07e81bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Auto_model_train(json_file):\n",
    "    \n",
    "    #important modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from striprtf.striprtf import rtf_to_text\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # extract the text from rtf file and parse it as a json\n",
    "    with open(json_file, 'r') as f:\n",
    "        rtf_content = f.read()\n",
    "        plain_text = rtf_to_text(rtf_content)\n",
    "        dic = json.loads(plain_text)\n",
    "        \n",
    "    \n",
    "    # read the csv as data frame\n",
    "    df = pd.read_csv(dic['design_state_data']['session_info']['dataset'])\n",
    "    \n",
    "    # target columns detail\n",
    "    target_col_type = dic['design_state_data']['target']['prediction_type']\n",
    "    target_col = dic['design_state_data']['target']['target']\n",
    "    \n",
    "    # extract the columns name that is selected true\n",
    "    org_df_col = df.columns\n",
    "    is_false_col = []\n",
    "    \n",
    "    \n",
    "    for col in org_df_col:\n",
    "        if not dic['design_state_data']['feature_handling'][col]['is_selected']:\n",
    "            is_false_col.append(col)\n",
    "            \n",
    "    # drop not_selected_col:\n",
    "    df.drop(columns = is_false_col, inplace = True)\n",
    "    \n",
    "    # split the data frame into x and y\n",
    "    x = df.drop(columns = target_col).copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # numeric columns and categorical columns in x\n",
    "    num_col_x = x.select_dtypes(exclude = 'O').columns\n",
    "    cat_col_x = x.select_dtypes(include = 'O').columns\n",
    "    \n",
    "    \n",
    "    # extract the columns name that's missing vlaues will be imputed and perfrom scaling\n",
    "    avg_col = []\n",
    "    median_col = []\n",
    "    mode_col = []\n",
    "    std_scaling_col = []\n",
    "    min_max_scaling_col =[]\n",
    "    \n",
    "    for col in num_col_x:\n",
    "        \n",
    "        \n",
    "        for key in dic['design_state_data']['feature_handling'][col]['feature_details']:\n",
    "            \n",
    "            if key == 'rescaling' and dic['design_state_data']['feature_handling'][col]['feature_details'][key] == 'MinMaxScaler':\n",
    "                min_max_scaling_col.append(col)\n",
    "                \n",
    "            elif key == 'rescaling' and dic['design_state_data']['feature_handling'][col]['feature_details'][key] == 'StandardScaler':\n",
    "                std_scaling_col.append(col)\n",
    "            \n",
    "            # if missing_values false then loop start from starts\n",
    "            elif key == 'missing_values' and not dic['design_state_data']['feature_handling'][col]['feature_details'][key]:\n",
    "                continue\n",
    "            \n",
    "            elif key == 'impute_with' and dic['design_state_data']['feature_handling'][col]['feature_details'][key] == 'Average of values':\n",
    "                avg_col.append(col)\n",
    "                \n",
    "            elif key == 'impute_with' and dic['design_state_data']['feature_handling'][col]['feature_details'][key] == 'Median of values':\n",
    "                median_col.append(col)\n",
    "                \n",
    "            elif key == 'impute_with' and dic['design_state_data']['feature_handling'][col]['feature_details'][key] == 'Mode of values':\n",
    "                mode_col.append(col)\n",
    "            \n",
    "            \n",
    "    # split the data set into train test split\n",
    "    r_s = dic['design_state_data']['train']['random_seed']\n",
    "    train_ratio = dic['design_state_data']['train']['train_ratio']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = tts(x,y, train_size = train_ratio, random_state = r_s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define object of simpleimputer\n",
    "    si_mean = SimpleImputer(strategy = 'mean')\n",
    "    si_median = SimpleImputer(strategy = 'median')\n",
    "    si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "    # impute missing values \n",
    "    if len(avg_col) > 0:\n",
    "        x_train[avg_col] = si_mean.fit_transform(x_train[avg_col])\n",
    "        x_test[avg_col]  = si_mean.transform(x_test[avg_col])\n",
    "    \n",
    "    if len(median_col) > 0:\n",
    "        x_train[median_col] = si_median.fit_transform(x_train[median_col])\n",
    "        x_test[median_col]  = si_median.transform(x_test[median_col])\n",
    "        \n",
    "    if len(mode_col) > 0:\n",
    "        x_train[mode_col] = si_mode.fit_transform(x_train[mode_col])\n",
    "        x_test[mode_col]  = si_mode.transform(x_test[mode_col])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # define object of scaler \n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    std_scaler = StandardScaler()\n",
    "    # rescaling the features \n",
    "    if len(std_scaling_col) > 0:\n",
    "        x_train[std_scaling_col] = std_scaler.fit_transform(x_train[std_scaling_col])\n",
    "        x_test[std_scaling_col]  = std_scaler.transform(x_test[std_scaling_col])\n",
    "        \n",
    "    if len(min_max_scaling_col) > 0:\n",
    "        x_train[min_max_scaling_col] = min_max_scaler.fit_transform(x_train[min_max_scaling_col])\n",
    "        x_test[min_max_scaling_col]  = min_max_scaler.transform(x_test[min_max_scaling_col])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # deifne the object of onehotencoder\n",
    "    Ot = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "    # handel categorical columns if have any\n",
    "    if len(cat_col_x) > 0:\n",
    "        \n",
    "        # encoding cat col and add in data set \n",
    "        x_train[ot.get_feature_names_out()] = ot.fit_transform(x_train[cat_col_x])\n",
    "        x_test[ot.get_feature_names_out()] = ot.transform(x_test[cat_col_x])\n",
    "        \n",
    "        # drop cat col after encoding it \n",
    "        x_train.drop(columns = cat_col_x, inplace = True)\n",
    "        x_test.drop(columns = cat_col_x, inplace = True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # define the object of label encoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # if y (target columns) is a categorical columns then perfrom label encoder\n",
    "    if target_col_type == 'Classification' and y.dtype == 'O':\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_test = le.transform(y_test)\n",
    "        \n",
    "        # inverse of label encoder\n",
    "        labels = pd.DataFrame()\n",
    "        labels['Value'] = pd.Series(y_train).unique()\n",
    "        labels['Label'] = le.inverse_transform(labels.Value.values)\n",
    "        labels.set_index('Value', inplace = True)\n",
    "        \n",
    "    \n",
    "    \n",
    "   # found the algoritham that will use to train the moel  \n",
    "    algo_df = pd.DataFrame({'Regression':['RandomForestRegressor','DecisionTreeRegressor'],\n",
    "                           'Classification':['RandomForestClassifier','DecisionTreeClassifier']})\n",
    "    \n",
    "    algo_list = []\n",
    "    \n",
    "    for algo in algo_df[target_col_type]:\n",
    "        if dic['design_state_data']['algorithms'][algo]['is_selected']:\n",
    "            algo_list.append(algo)\n",
    "        \n",
    "    \n",
    "    # define result df for regression model\n",
    "    result_df = pd.DataFrame({'algoritham':[],\n",
    "                              'r2_train':[],\n",
    "                              'r2_test':[],\n",
    "                              'adj_r2_train':[],\n",
    "                              'adj_r2_test':[],\n",
    "                              'rmse_train':[],\n",
    "                              'rmse_test':[]})\n",
    "    \n",
    "    # train test row and columns\n",
    "    train_n, train_k = x_train.shape\n",
    "    test_n, test_k   = x_test.shape\n",
    "    \n",
    "    # RandomForestRegressor Model\n",
    "    if 'RandomForestRegressor' in algo_list:\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "    \n",
    "        # base model define \n",
    "        base_model_rfr = RandomForestRegressor()\n",
    "        \n",
    "        # parmeters values list \n",
    "        n_estimators_list =  pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestRegressor']['min_trees'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestRegressor']['max_trees'],4)).astype('int').values\n",
    "        \n",
    "        max_depth_list =  pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestRegressor']['min_depth'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestRegressor']['max_depth'],4)).astype('int').values\n",
    "        \n",
    "        \n",
    "        min_samples_leaf_list =  pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestRegressor']['min_samples_per_leaf_min_value'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestRegressor']['min_samples_per_leaf_max_value'],4)).astype('int').values\n",
    "        # parameters dict\n",
    "        parameters_rfr = {'n_estimators':n_estimators_list,\n",
    "                     'max_depth':max_depth_list,\n",
    "                     'min_samples_leaf':min_samples_leaf_list}\n",
    "        \n",
    "        # object of GridSearchCV\n",
    "        \n",
    "        clf_r = GridSearchCV(base_model_rfr,parameters_rfr, scoring = 'neg_root_mean_squared_error')\n",
    "        clf_r.fit(x_train,y_train)\n",
    "        best_parm = clf_r.best_params_\n",
    "        \n",
    "        # final model\n",
    "        model_rfr_f = RandomForestRegressor(n_estimators = best_parm['n_estimators'], max_depth = best_parm['max_depth'],\n",
    "                                           min_samples_leaf = best_parm['min_samples_leaf'])\n",
    "        \n",
    "        # model train\n",
    "        model_rfr_f.fit(x_train,y_train)\n",
    "        \n",
    "        r2_train = r2_score(y_train, model_rfr_f.predict(x_train))\n",
    "        r2_test = r2_score(y_test, model_rfr_f.predict(x_test))\n",
    "        \n",
    "        rmse_train = root_mean_squared_error(y_train, model_rfr_f.predict(x_train))\n",
    "        rmse_test = root_mean_squared_error(y_test, model_rfr_f.predict(x_test))\n",
    "        \n",
    "        adj_r2_train = 1 - (((1-r2_train) *( train_n-1))/(train_n-train_k-1))\n",
    "        adj_r2_test = 1 - (((1-r2_test) *( test_n-1))/(test_n-test_k-1))\n",
    "        \n",
    "        temp_df = pd.DataFrame({'algoritham':['RandomForestRegressor'],\n",
    "                                'r2_train':[r2_train],\n",
    "                                'r2_test':[r2_test],\n",
    "                                'adj_r2_train':[adj_r2_train],\n",
    "                                'adj_r2_test':[adj_r2_test],\n",
    "                                'rmse_train':[rmse_train],\n",
    "                                'rmse_test':[rmse_test]})\n",
    "        \n",
    "        # concat the temp_df with result_df\n",
    "        result_df = pd.concat([result_df, temp_df])\n",
    "        \n",
    "        \n",
    "     # RandomClassifier Model\n",
    "    if 'RandomForestClassifier' in algo_list:\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.metrics import classification_report\n",
    "    \n",
    "        # base model define \n",
    "        base_model_rfc = RandomForestClassifier()\n",
    "        \n",
    "        # parmeters values list \n",
    "        n_estimators_list = pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestClassifier']['min_trees'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestClassifier']['max_trees'],4)).astype('int').values\n",
    "        \n",
    "        max_depth_list = pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestClassifier']['min_depth'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestClassifier']['max_depth'],4)).astype('int').values\n",
    "        \n",
    "        min_samples_leaf_list = pd.Series(np.linspace(dic['design_state_data']['algorithms']['RandomForestClassifier']['min_samples_per_leaf_min_value'],\n",
    "                                      dic['design_state_data']['algorithms']['RandomForestClassifier']['min_samples_per_leaf_max_value'],4)).astype('int').values\n",
    "        # parameters dict\n",
    "        parameters_rfc = {'n_estimators':n_estimators_list,\n",
    "                     'max_depth':max_depth_list,\n",
    "                     'min_samples_leaf':min_samples_leaf_list}\n",
    "        \n",
    "        \n",
    "        # object of GridSearchCV\n",
    "        clf_c = GridSearchCV(base_model_rfc,parameters_rfc, scoring = 'f1_macro')\n",
    "        clf_c.fit(x_train,y_train)\n",
    "        best_parm = clf_c.best_params_\n",
    "        \n",
    "        # final model\n",
    "        model_rfc_f = RandomForestClassifier(n_estimators = best_parm['n_estimators'], max_depth = best_parm['max_depth'],\n",
    "                                           min_samples_leaf = best_parm['min_samples_leaf'])\n",
    "        \n",
    "        # model train\n",
    "        model_rfc_f.fit(x_train,y_train)\n",
    "        \n",
    "        y_pred_train = model_rfc_f.predict(x_train)\n",
    "        y_pred_test = model_rfc_f.predict(x_test)\n",
    "        \n",
    "        print('Randomforest_classsifier ----------------------------------')\n",
    "        print(labels)\n",
    "        print('')\n",
    "        print(f\"Train_report:-- \")\n",
    "        print(classification_report(y_train,y_pred_train))\n",
    "        print(\"\")\n",
    "        print(f\"Test_report:-- \")\n",
    "        print(classification_report(y_test,y_pred_test))\n",
    "        \n",
    "    \n",
    "    \n",
    "    # decision tree classsifier\n",
    "    if 'DecisionTreeClassifier' in algo_list:\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        # base model define \n",
    "        base_model_dc = DecisionTreeClassifier()\n",
    "        \n",
    "        # parmeters values list\n",
    "        criterion_list = []\n",
    "        \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeClassifier']['use_gini']:\n",
    "            criterion_list.append('gini')\n",
    "            \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeClassifier']['use_entropy']:\n",
    "            criterion_list.append('entropy')\n",
    "        \n",
    "        \n",
    "        splitter_list = []\n",
    "        \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeClassifier']['use_best']:\n",
    "            splitter_list.append('best')\n",
    "            \n",
    "        \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeClassifier']['use_random']:\n",
    "            splitter_list.append('random')     \n",
    "        \n",
    "        \n",
    "        min_samples_per_leaf_list = dic['design_state_data']['algorithms']['DecisionTreeClassifier']['min_samples_per_leaf']\n",
    "        \n",
    "        max_depth_list =  pd.Series(np.linspace(dic['design_state_data']['algorithms']['DecisionTreeClassifier']['min_depth'],\n",
    "                                      dic['design_state_data']['algorithms']['DecisionTreeClassifier']['max_depth'],4)).astype('int').values \n",
    "            \n",
    "        # parameters dict\n",
    "        parameters_dc = {'criterion':criterion_list\n",
    "                         ,'splitter':splitter_list,\n",
    "                         'max_depth':max_depth_list,\n",
    "                         'min_samples_leaf':min_samples_per_leaf_list}\n",
    "                     \n",
    "        \n",
    "        # object of GridSearchCV\n",
    "        clf_dc = GridSearchCV(base_model_dc,parameters_dc, scoring = 'f1_macro')\n",
    "        clf_dc.fit(x_train,y_train)\n",
    "        best_parm_dc = clf_dc.best_params_\n",
    "        \n",
    "        \n",
    "        # final model\n",
    "        model_dc_f = DecisionTreeClassifier(criterion = best_parm_dc['criterion'], max_depth = best_parm_dc['max_depth'],\n",
    "                                           min_samples_leaf = best_parm_dc['min_samples_leaf'],splitter = best_parm_dc['splitter'])\n",
    "        \n",
    "        # model train\n",
    "        model_dc_f.fit(x_train,y_train)\n",
    "        \n",
    "        y_pred_train = model_dc_f.predict(x_train)\n",
    "        y_pred_test = model_dc_f.predict(x_test)\n",
    "        \n",
    "        print('DecisionTreeClassifier ----------------------------------')\n",
    "        print(labels)\n",
    "        print('')\n",
    "        print(f\"Train_report:-- \")\n",
    "        print(classification_report(y_train,y_pred_train))\n",
    "        print(\"\")\n",
    "        print(f\"Test_report:--\")\n",
    "        print(classification_report(y_test,y_pred_test))\n",
    "        \n",
    "        \n",
    "\n",
    "    # decision tree regressior\n",
    "    if 'DecisionTreeRegressor' in algo_list:\n",
    "        \n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "        \n",
    "        # base model define\n",
    "        base_model_dtr = DecisionTreeRegressor()\n",
    "        \n",
    "        # parmeters values list\n",
    "        min_samples_per_leaf_list = dic['design_state_data']['algorithms']['DecisionTreeRegressor']['min_samples_per_leaf']\n",
    "        \n",
    "        max_depth_list =  pd.Series(np.linspace(dic['design_state_data']['algorithms']['DecisionTreeRegressor']['min_depth'],\n",
    "                                      dic['design_state_data']['algorithms']['DecisionTreeRegressor']['max_depth'],4)).astype('int').values \n",
    "        \n",
    "        \n",
    "        splitter_list = []\n",
    "        \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeRegressor']['use_best']:\n",
    "            splitter_list.append('best')\n",
    "            \n",
    "        \n",
    "        if dic['design_state_data']['algorithms']['DecisionTreeRegressor']['use_random']:\n",
    "            splitter_list.append('random')\n",
    "        \n",
    "        # parameters dict\n",
    "        parameters_dtr = {'splitter':splitter_list,\n",
    "                         'max_depth':max_depth_list,\n",
    "                         'min_samples_leaf':min_samples_per_leaf_list}\n",
    "        \n",
    "        # object of GridSearchCV\n",
    "        clf_dtr = GridSearchCV(base_model_dtr,parameters_dtr, scoring = 'neg_root_mean_squared_error')\n",
    "        clf_dtr.fit(x_train,y_train)\n",
    "        best_parm_dtr = clf_dtr.best_params_\n",
    "        \n",
    "        # final model\n",
    "        model_dtr_f = DecisionTreeRegressor(max_depth = best_parm_dtr['max_depth'],\n",
    "                                           min_samples_leaf = best_parm_dtr['min_samples_leaf'],splitter = best_parm_dtr['splitter'])\n",
    "        \n",
    "        # model train\n",
    "        model_dtr_f.fit(x_train,y_train)\n",
    "        \n",
    "        r2_train = r2_score(y_train, model_dtr_f.predict(x_train))\n",
    "        r2_test = r2_score(y_test, model_dtr_f.predict(x_test))\n",
    "        \n",
    "        rmse_train = root_mean_squared_error(y_train, model_dtr_f.predict(x_train))\n",
    "        rmse_test = root_mean_squared_error(y_test, model_dtr_f.predict(x_test))\n",
    "        \n",
    "        adj_r2_train = 1 - (((1-r2_train) *( train_n-1))/(train_n-train_k-1))\n",
    "        adj_r2_test = 1 - (((1-r2_test) *( test_n-1))/(test_n-test_k-1))\n",
    "        \n",
    "        temp_df = pd.DataFrame({'algoritham':['DecisionTreeRegressor'],\n",
    "                                'r2_train':[r2_train],\n",
    "                                'r2_test':[r2_test],\n",
    "                                'adj_r2_train':[adj_r2_train],\n",
    "                                'adj_r2_test':[adj_r2_test],\n",
    "                                'rmse_train':[rmse_train],\n",
    "                                'rmse_test':[rmse_test]})\n",
    "        \n",
    "        \n",
    "        # concat the temp_df with result_df\n",
    "        result_df = pd.concat([result_df, temp_df])\n",
    "        \n",
    "    # return result df if regression problem\n",
    "    if target_col_type == 'Regression':\n",
    "        return result_df\n",
    "    \n",
    "                         \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35423ef6-b094-4b0e-9e80-be4f166b5499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomforest_classsifier ----------------------------------\n",
      "                 Label\n",
      "Value                 \n",
      "1      Iris-versicolor\n",
      "2       Iris-virginica\n",
      "0          Iris-setosa\n",
      "\n",
      "Train_report:-- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.95      0.95      0.95        37\n",
      "           2       0.95      0.95      0.95        43\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.97      0.97      0.97       120\n",
      "weighted avg       0.97      0.97      0.97       120\n",
      "\n",
      "\n",
      "Test_report:-- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.92      0.85      0.88        13\n",
      "           2       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.89      0.90      0.89        30\n",
      "weighted avg       0.91      0.90      0.90        30\n",
      "\n",
      "DecisionTreeClassifier ----------------------------------\n",
      "                 Label\n",
      "Value                 \n",
      "1      Iris-versicolor\n",
      "2       Iris-virginica\n",
      "0          Iris-setosa\n",
      "\n",
      "Train_report:-- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.97      0.89      0.93        37\n",
      "           2       0.91      0.98      0.94        43\n",
      "\n",
      "    accuracy                           0.96       120\n",
      "   macro avg       0.96      0.96      0.96       120\n",
      "weighted avg       0.96      0.96      0.96       120\n",
      "\n",
      "\n",
      "Test_report:--\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.95      0.93        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Auto_model_train('algoparams_from_ui1.json.rtf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d928d1b-3d43-48b6-9d36-860e14ad96aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81041e99-e164-489f-9372-5c8a10487817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dada57-d8e7-448f-9912-8631ea7b9dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
